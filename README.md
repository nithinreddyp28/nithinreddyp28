# Data Science Portfolio - Nithin Reddy Penta Reddy

This portfolio showcases my work in **Data Analytics**, **Machine Learning**, **Financial Modeling**, **M&A**, and **Investment Banking**. It includes my academic projects, personal projects, and achievements in the field of data science and analytics.

- **Email**: [pentareddynithin@gmail.com](mailto:pentareddynithin@gmail.com)
- **LinkedIn**: [linkedin.com/in/nithinpenta](https://www.linkedin.com/in/nithinpenta/)
- **Kaggle**: [kaggle.com/nithinreddyp28](https://www.kaggle.com/nithinreddyp28)

## üöÄ Current Focus
- **Machine Learning**: Applying ML techniques to real-world business problems.
- **Data Analytics**: Using data-driven insights for better business decisions.
- **Predictive Modeling**: Building and deploying predictive models to improve business outcomes.

## üíº Recent Experience

### **Sr. Data Engineer / BI Enginee**  
**LGS, Montreal, CA** ‚Äî *Sep 2024 ‚Äì Present*
- Architected and scaled production-grade ETL/ELT pipelines using Databricks Lakehouse (Delta Lake, Delta Live Tables, Autoloader, Workflows), Azure Data Factory, Apache Airflow, and Microsoft Fabric, ingesting high-volume structured, semi-structured, and NoSQL data into Snowflake with improved SLA reliability.
- Designed hybrid ETL/ELT architectures, executing compute-intensive transformations in Databricks/Spark while optimizing analytics workloads in Snowflake via dbt, balancing performance, scalability, and cost efficiency.
- Led migration of legacy Informatica PowerCenter workflows to Informatica Intelligent Data Management Cloud (IDMC), rebuilding complex mappings, taskflows, scheduling, parameterization, and centralized error handling aligned with cloud-native execution patterns.
- Designed and enhanced Workday integrations using Informatica IDMC connectors and REST APIs, implementing incremental loads, CDC logic, reconciliation checks, and audit controls across Workday HCM and Financials data.
- Engineered AWS-based data architectures leveraging S3, EC2, RDS, IAM, VPC, and CloudWatch, including AWS Glue ETL workflows and AWS Lambda-driven serverless pipelines for event-based ingestion and low-latency processing.
- Implemented secure cross-cloud integrations across Azure, Informatica IDMC, and AWS using IAM roles, encryption-at-rest/in-transit, VPC networking, and enterprise credential management standards.
- Solved complex data engineering challenges using Python (PySpark, Pandas, NumPy), dbt, and Great Expectations, building scalable transformations, validation frameworks, and production-grade batch and streaming pipelines.
- Optimized Snowflake performance using Snowpipe, Streams, Tasks, clustering strategies, and SQL refactoring, reducing analytical query runtimes by 30%.
- Designed and operationalized real-time, event-driven pipelines using Kafka, Spark Structured Streaming, Azure Service Bus, AWS Lambda, and API-based integrations (REST, SOAP, GraphQL).
- Standardized enterprise ingestion and transformation frameworks through data contracts, schema validation, CDC logic, reconciliation workflows, and freshness SLAs, enforcing quality gates via Great Expectations and dbt tests.
- Established analytics-ready Bronze/Silver/Gold datasets with SCD Type 2 modeling, enabling BI and self-service analytics across Power BI, Tableau, Looker, and Looker Studio.
- Performed Spark UI-driven performance debugging, resolving bottlenecks related to shuffle operations, skewed partitions, memory pressure, and inefficient joins.
- Strengthened governance and security posture via Unity Catalog, RBAC, Azure AD controls, Key Vault, lineage tracking, audit logging, and HIPAA-aligned practices.
- Reduced cloud compute and storage costs by 25% through Databricks cluster optimization, Snowflake credit governance, and Azure storage lifecycle strategies.
- Enabled advanced analytics and AI-driven use cases by developing Python-based microservices using Flask, FastAPI, TensorFlow, and scikit-learn, supporting anomaly detection, drift monitoring, and NLP-driven workflows.
- Improved platform reliability and observability through SLO-based monitoring, lineage tracking, anomaly detection, and Airflow-driven orchestration.
- Automated CI/CD pipelines across Databricks, Snowflake, dbt, IDMC, and Fabric using Azure DevOps, GitHub Actions, Jenkins, GitLab CI, and Terraform.
- Built reusable platform utilities in Python and Golang, standardizing ingestion, validation, monitoring, orchestration, and IaC provisioning.
- Acted as technical SME for IDMC, Workday integrations, and ETL modernization initiatives, leading client workshops and solution design sessions.

### **Sr BI Engineer/ETL Developer**  
Verity Knowledge Solutions Pvt Ltd (UBS) ‚Äî *Sep 2018 ‚Äì Aug 2023*
- Supported healthcare, finance, investment banking, retail, and ecommerce analytics by engineering scalable pipelines using Azure Data Factory, Synapse, Snowflake, Databricks, SSIS, and Informatica (PowerCenter, IDQ, IICS, IDMC).
- Designed high-throughput ingestion frameworks leveraging Azure Blob Storage, ADLS Gen2, SQL staging layers, SSIS data flows, dbt models, and Fabric Data Factory pipelines.
- Improved analytical performance through dimensional modeling, star schemas, and Snowflake micro-partition optimization aligned with Kimball methodology.
- Developed dashboards and semantic models in Power BI, Tableau, Looker, and Looker Studio, enabling KPI frameworks, forecasting, and decision-support analytics.
- Reduced BI defects by 45% by implementing MDM and data quality frameworks using Informatica IDQ, SQL, and validation pipelines.
- Delivered ML capabilities including classification, regression, clustering, forecasting, and time-series models using TensorFlow and scikit-learn, deployed via Flask/FastAPI APIs.
- Built NLP pipelines using TF-IDF, LDA, spaCy, NLTK, and BERT for document tagging, sentiment analysis, and metadata extraction.
- Strengthened data science workflows via GLM analysis, hypothesis testing, feature engineering, and statistical profiling.
- Increased ingestion reliability through REST, SOAP, XML, JSON, Salesforce, vendor APIs, and SFTP integrations with automated validation and reconciliation.
- Reduced manual workload by over 70% by developing reusable SQL, SSIS, dbt, and PySpark components.
- Modernized reporting systems through SSIS, SSRS, and SSAS tabular/OLAP models.
- Automated deployment workflows using Azure DevOps, GitHub Actions, Jenkins, GitLab CI, and Terraform.
- Advanced enterprise BI maturity through Microsoft Fabric Lakehouse and centralized semantic modeling.
- Built Reverse ETL pipelines delivering curated datasets into CRM, ERP, and operational systems.
- Strengthened ecommerce analytics via funnel analysis, attribution modeling, retention modeling, and UA ‚Üí GA4 migrations.
- Developed metadata-driven ingestion and transformation frameworks, reducing pipeline build time by 60%.
- Led multi-team initiatives aligning modeling standards, governance practices, and analytics roadmaps.
- Mentored junior engineers on data modeling, dbt, analytics engineering, and visualization best practices.


## ‚öôÔ∏è Skills & Tools
- **Cloud Platforms and Services**: Azure (ADF, ADLS Gen2, Blob, Synapse, Azure SQL DB, Azure SQL DW, Service Bus, AAS, AAD, DMS), AWS (S3, EC2, RDS, IAM, VPC, Glue, Lambda, CloudWatch), MS Fabric (OneLake, Fabric DF), Azure DevOps, Azure Storage Explorer, IaaS Provisioning, Pipeline Perf Optimization (ADF / SSIS Caching).
- **Big Data, ELT and Pipeline Engineering**: Databricks (Workflows, DLT, Autoloader, UC), Spark (Structured Streaming), Delta Lake, Kafka, dbt, Fivetran, AWS Glue, AWS Lambda, Informatica (PowerCenter, IDQ, IICS, IDMC), SSIS, Airflow, Reverse ETL, API Ingestion (REST, SOAP, GraphQL), SFTP, SaaS Integrations (Workday), Snowflake (Streams, Tasks, Snowpipe), Oracle DW, AWS-Hosted Oracle Systems.
- **Programming and Scripting**: Python, PySpark, Golang, SQL (ANSI SQL, T-SQL), R, C#, PowerShell, Bash, Pandas, NumPy, Regex, Matplotlib, scikit-learn.
Databases and Storage Systems: SQL Server, Azure SQL DB, Synapse, Snowflake, MySQL, PostgreSQL, Cosmos DB, MongoDB, Delta Lake Tables, ADLS / Blob Staging, Data Lake Zones.
- **Machine Learning and NLP**: TensorFlow, scikit learn, Natural Language Processing using TF IDF, LDA, NLTK, spaCy, BERT, Model deployment through Flask and FastAPI, containerization using Docker and Kubernetes, Statistical modeling including regression, clustering, GLM, hypothesis testing, forecasting and time series analysis.
- **Business Intelligence and Visualization**: Power BI, Tableau, Looker, Looker Studio, SSRS, SSAS (tabular and OLAP cubes), Visual Studio, Power Query M Language, semantic modeling, KPI frameworks, star schema and Snowflake schema design, Architected medallion architectures (Bronze/Silver/Gold) with SCD Type 2 modeling, Spark UI‚Äìdriven performance optimization, and analytics layers supporting decision velocity, product analytics, and business performance reporting
- **Data Modeling and Architecture**: Star schema, Snowflake schema, fact and dimension modeling, Kimball methodology, OLAP cube design, semantic layer design, MDM principles, data quality and governance frameworks including lineage, RBAC, key vault integration and HIPAA aligned data management, Data contracts, SLAs/SLOs, domain-driven data products, Data Mesh patterns, contract testing, schema evolution governance.
- **DevOps, CI/CD and Infrastructure as Code**: Azure DevOps pipelines, GitHub Actions, GitLab CI, Jenkins, YAML-based deployments, ARM templates, Terraform, infrastructure as code practices, automated deployments for ADF, dbt, Databricks, Power BI and containerized services, SLA/SLO dashboards, anomaly detection, lineage, pipeline reliability engineering.
- **Tools and Utilities**: SQL Server Management Studio, Business Intelligence Development Studio, Visual Studio, SQL Profiler, Import and Export Wizard, Synapse Studio, Fabric Data Factory, Databricks Repos, Jupyter notebooks, monitoring dashboards and metadata-driven ETL frameworks.
- **Operating Systems**: Windows, Windows Server, Ubuntu.
- **Project Management & Collaboration:**: Agile methodology, cross-functional teamwork, executive-level reporting

## üìä Notable Projects

### 1. [Crisis Management and Public Safety (Toronto)](https://github.com/nithinreddyp28/A-Machine-Learning-Approach-to-Crisis-Management-and-Public-Safety-in-Toronto)**
This project leverages machine learning to improve crisis management in Toronto by analyzing crisis event data to predict patterns and apprehensions. By examining event types, time of occurrence, and neighborhood locations, the study provides insights to enhance public safety and response strategies.
- Classification: Random Forest, Logistic Regression, Gradient Boosting.
- Clustering: K-Means for high-risk neighborhood identification.
Key Techniques:
- Binary Encoding: Apprehension status as a binary variable.
- Cyclical Encoding: Encoding time-based features for better pattern recognition.

### 2. **[Predictive Modeling with ML:  Customer Churn Prediction: Unlocking Insights into Customer Retention](https://github.com/nithinreddyp28/Customer-Churn-Prediction)**
This project explores customer churn prediction using machine learning, focusing on analyzing customer demographics, account details, and behavioral data. The goal is to predict whether a telecom customer will churn, helping businesses improve retention strategies. Various models, including Random Forest, Gradient Boosting, Neural Networks, SVM, and KNN, were employed for predictive analysis.
Key Techniques:
- Binary Encoding: Converted categorical variables into numerical format.
- SMOTE (Synthetic Minority Over-sampling Technique): Balanced the dataset for better model performance.
- Cyclical Feature Encoding: Applied sine & cosine transformations for time-based patterns.
- Hyperparameter Tuning: Optimized models for improved accuracy and recall
Tools & Technologies:
- PySpark: Scalable data processing and ML model training.
- Python Libraries:
- pandas & NumPy ‚Äì Data manipulation and numerical operations.
- matplotlib & seaborn ‚Äì Data visualization.
- sklearn ‚Äì Machine learning models (Random Forest, SVM, KNN).
- imblearn ‚Äì Implementing SMOTE for class balancing.
- tensorflow ‚Äì Building and training Neural Networks.

### 3. **[Vehicle Collision Analysis](https://github.com/nithinreddyp28/Vehicle-Collision-Analysis)**
The Motor Vehicle Collisions Analysis - NYC project examines traffic accidents in New York City using NYPD collision data. The study identifies key factors contributing to injuries and fatalities, highlights high-risk areas, and applies predictive modeling to improve road safety. Key findings reveal that Brooklyn and Queens have the highest fatalities, while distraction, right-of-way violations, and unsafe speed are major contributing factors. Regression models and statistical tests validate these insights, supporting data-driven safety initiatives like Vision Zero.
Tools & Technologies:
- Programming Languages: Python
- Libraries & Frameworks: pandas, NumPy(Data manipulation), matplotlib, seaborn (Data visualization)
- scikit-learn ‚Äì Machine learning models (Logistic Regression, Ridge, Lasso)
- statsmodels ‚Äì Statistical tests (ANOVA, Chi-Square)
Techniques Applied:
- Predictive Modeling (GLM Logistic Regression, Ridge, Lasso)
- Statistical Tests (ANOVA, Chi-Square)

### 4. **[Employee Management System](https://github.com/nithinreddyp28/SQL-Database-for-Employee-Management)**
The SQL Database Project: Employee Management System is a Payroll Management System (PMS) designed to streamline payroll operations, employee management, tax compliance, and reporting. It automates payroll calculations, tax deductions, and payment processing while ensuring data security through role-based access control. The system manages employee details, department assignments, time tracking, tax calculations, and reporting. The database design follows 1:1, 1:M, and M:M relationships, enabling efficient management of employees, payroll, attendance, and finance operations.
Tools & Technologies:
- Database Management System (DBMS): MySQL, PostgreSQL, or SQL Server
SQL Concepts Applied:
- Entity-Relationship (ER) Modeling ‚Äì Designing relationships between employees, payroll, attendance, and departments
- Normalization ‚Äì Ensuring data integrity and eliminating redundancy
- Stored Procedures & Triggers ‚Äì Automating payroll calculations and tax deductions
- Role-Based Access Control (RBAC) ‚Äì Implementing user authentication and security
- Querying & Reporting: SQL queries for payroll reports, tax summaries, and attendance tracking

## üèÜ Certifications
- **Data Science with R** by SimplyLearn ([Certificate](https://certificates.simplicdn.net/share/4539569.pdf))
- **Data Analytics with Python** by IBM ([Certificate](https://courses.skillsnet.simplilearn.com/certificates/534f4ff99bce4a43acf30ca48b0a7177))
- **Analytics Foundation with Excel** by SimplyLearn ([Certificate](https://certificates.simplicdn.net/share/4264561.pdf))

## üéØ Accomplishments
- Recognized by the Open Data Team's Marketing and Communications Lead for my analysis of the "Persons in Crisis Calls for Service Attended" dataset, with an invitation to publish my work on their gallery page.
- Performer of the year in 2017, 2019, and 2022 at Verity Knowledge Solutions.

## üå± Currently Learning
- Python, R, SQL, and Data Analytics
- Advanced Machine Learning Techniques

## üí¨ Let's Connect!
Feel free to reach out or connect with me on:
- [LinkedIn](https://www.linkedin.com/in/nithin-reddy-penta-reddy-32093bb9)
- [Kaggle](https://www.kaggle.com/nithinreddyp28)
- Email: [pentareddynithin@gmail.com](mailto:nithinreddy28@gmail.com)

Thanks for visiting my portfolio! üôå

---
Made with ‚ù§Ô∏è
